{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting anomalies in credit card transaction data (Python, scikit-learn)\n",
    "\n",
    "Author: [Dennis W. Hallema](https://www.linkedin.com/in/dennishallema) \n",
    "\n",
    "Description: Supervised classification procedure for detecting fraudulous credit card transactions in a large dataset. This procedure compares the performance of four classifiers: Logistic Regression, Kernel Support Vector Classifier, Stochastic Gradient Boosting and Random Forest. \n",
    "\n",
    "Dependencies: See `environment.yml`. \n",
    "\n",
    "Data: PCA transformed credit card transaction data collected in Europe over the course of two days. This anonymized dataset was created by Worldline and the Machine Learning Group of Universit√© Libre de Bruxelles (http://mlg.ulb.ac.be). \n",
    "\n",
    "Disclaimer: Use at your own risk. No responsibility is assumed for a user's application of these materials or related materials. \n",
    "\n",
    "References: \n",
    "\n",
    "* Dal Pozzolo, A., Caelen, O., Le Borgne, Y-A, Waterschoot, S. \\& Bontempi, G. (2014). Learned lessons in credit card fraud detection from a practitioner perspective. Expert Systems with Applications, 41(10), 4915-4928. \n",
    "\n",
    "* Dal Pozzolo, A., Boracchi, G., Caelen, O., Alippi, C. \\& Bontempi, G. (2018). Credit card fraud detection: a realistic modeling and a novel learning strategy. IEEE Transactions on Neural Networks and Learning Systems, 29(8), 3784-3797.\n",
    "\n",
    "Content: \n",
    "\n",
    "* [Data preparation](#one) \n",
    "* [Logistic Regression (LR)](#two) \n",
    "* [Kernel SVM classification (SVC)](#three) \n",
    "* [Gradient Boosting Model (GBM) classification](#four) \n",
    "* [Random Forest (RF) classification](#five) \n",
    "* [Model selection and cost-effective optimization](#six) \n",
    "* [Conclusion](#seven) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation <a id='one'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('data/creditcard.csv', header=None)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print data types\n",
    "print(df.dtypes)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X,y\n",
    "X = df.iloc[:,:-2]\n",
    "y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram\n",
    "yhist = plt.hist(y)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('y Distribution')\n",
    "\n",
    "# Count transactions\n",
    "pos = sum(y)\n",
    "pos_rel = sum(y)/y.shape[0]\n",
    "print(\"Number of transactions: {}\".format(y.shape[0]))\n",
    "print(\"Anomalies: {}\".format(pos))\n",
    "print(\"Anomalies percentage of all transactions: %.4f%%\" % (pos_rel*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Summary of the credit card transaction dataset:* \n",
    "\n",
    "* The variables are unnamed, because we are not working with original credit card data but with variables that have been orthogonally transformed into uncorrelated variables (principal components). \n",
    "\n",
    "* There are 29 variables of type float and 1 variable of type integer. The former are the principal component that we can use as features, and the latter is the binary response variable indicating the transaction anomalies. \n",
    "\n",
    "* The number of anomalies is very small compared to the total number of transactions collected over the course of two days. In other words, the dataset is highly unbalanced, and this requires special attention when we build a classifier to predict anomalies in the transaction data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression <a id='two'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting anomalies is a binary classification problem. We assume that a transaction represents either an anomaly (1) or not (0), but never both. Logistic Regression is a good starting point for this type of classification because it is fast, and still allows us to explain what variables are influential. (While this is always the case, note that our variables are PCAs meaning that their influence does not give us any information.) We will follow a step-wise approach: \n",
    "\n",
    "1. Split the data into a training set and a testing (or hold-out) set; \n",
    "2. Scale and center the data; \n",
    "3. Fit an initial classifier: \n",
    "    * Use default parameters; \n",
    "    * Predict (non)anomalous transactions;  \n",
    "    * Evaluate initial classifier; \n",
    "4. Hyperparameter tuning of classifier with k-fold cross-validation: \n",
    "    * Identify optimized parameter set for classifier; \n",
    "    * Predict (non)anomalous transactions; \n",
    "    * Evaluate optimized classifier. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from inspect import signature\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, mean_squared_error, average_precision_score, precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing sets\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size = 0.3, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and center data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate classifier\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=200, random_state=21)\n",
    "\n",
    "# Fit classifier to the training set\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute training metrics\n",
    "accuracy = clf.score(X_train, y_train)\n",
    "\n",
    "#  Predict labels of test set\n",
    "train_pred = clf.predict(X_train)\n",
    "\n",
    "# Compute MSE, confusion matrix, classification report\n",
    "mse = mean_squared_error(y_train, train_pred)\n",
    "conf_mat = confusion_matrix(y_train.round(), train_pred.round())\n",
    "clas_rep = classification_report(y_train.round(), train_pred.round())\n",
    "\n",
    "# Print reports\n",
    "print('{:=^80}'.format('Initial LR training report'))\n",
    "print('Accuracy: %.4f' % accuracy)\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print(\"Confusion matrix:\\n{}\".format(conf_mat))\n",
    "print(\"Classification report:\\n{}\".format(clas_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute testing metrics\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "# Predict labels of test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute MSE, confusion matrix, classification report\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test.round(), y_pred.round())\n",
    "clas_rep = classification_report(y_test.round(), y_pred.round())\n",
    "\n",
    "# Print reports\n",
    "print('{:=^80}'.format('Initial LR testing report'))\n",
    "print('Accuracy: %.4f' % accuracy)\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print(\"Confusion matrix:\\n{}\".format(conf_mat))\n",
    "print(\"Classification report:\\n{}\".format(clas_rep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <td>Prediction: 0</td>\n",
    "    <td>Prediction: 1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Actual: 0 </td>\n",
    "    <td>True negative</td>\n",
    "    <td>False positive</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Actual: 1</td>\n",
    "    <td>False negative</td>\n",
    "    <td>True positive</td>\n",
    "  </tr>\n",
    "</table> \n",
    "\n",
    "* Precision = tp / (tp + fp) \n",
    "* Recall = tp / (tp + fn) \n",
    "* F-beta score = 2 * (precision * recall) / (precision + recall) \n",
    "\n",
    "The classification report (above) shows that the accuracy of the model is outstanding (close to 1.00). But this does not mean this a good model. Why? The vast majority - 99.83% of the transactions are not marked as anomalies in the dataset. An alternative model would simply classify all values as 0 (not anomalous), and still have an accuracy of 1.00 (98.83% to be exact). Despite the fact that this classifier has a very high accuracy and a very low mean squared error, we need to evaluate the metrics that reflect the fact that this dataset is highly unbalanced. We want to focus particularly on the class of interest: anomalous transactions. The recall rate for anomalies (value 1), is not indeed very high (0.59). While the precision for anomalies (0.89) is a good result for an uncalibrated model, the confusion matrix shows that we incorrectly classified 61 transactions as anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probabilities\n",
    "y_pred_prob = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculate receiver operating characteristics (ROC)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Compute AUC score\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k-')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Initial LR Testing\\nROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute AUPRC score\n",
    "average_precision = average_precision_score(y_test, y_pred_prob)\n",
    "print(\"AUPRC: {}\".format(average_precision))\n",
    "\n",
    "# Plot PR curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Initial LR Testing\\n2-class Precision-Recall curve: AP={0:0.4f}'.format(average_precision))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see that metrics like area under ROC curve and accuracy (above) give a too optimistic impression of model performance. To reflect the unbalanced character of the credit card transaction data (0.1727% of the transactions were anomalous and 99.8273‚Ä¨% were not anomalous), we also plotted the Precision-Recall curve. The area under the Precision-Recall curve (AUPRC) is a useful metric: if we had to assign this model a grade, 78/100 would be it. Not bad, but still much unexploited potential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the performance of the Logistic Regression classifier, we will calibrate its main parameter C with a random grid search. The C-parameter fixes the inverse of regularization strength, and setting this parameter to a smaller value will increase the regularization strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "c_space = np.logspace(-3, 2, 51)\n",
    "rand_grid = {'C': c_space,\n",
    "             'solver': ['lbfgs'] }\n",
    "print(rand_grid)\n",
    "\n",
    "# Instantiate search object (use all cores but one)\n",
    "grid = RandomizedSearchCV(LogisticRegression(random_state=21, max_iter=100), rand_grid,\n",
    "                          n_iter = 20, cv=2, random_state=21, n_jobs = -2, verbose = 2)\n",
    "\n",
    "# Fit object to data\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Extract best model\n",
    "optimized_clf = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the tuned parameters and score\n",
    "print('{:=^80}'.format('LR parameters for best candidate'))\n",
    "print(\"Optimized Parameters: {}\".format(grid.best_params_)) \n",
    "print(\"All Parameters: {}\".format(optimized_clf.get_params())) \n",
    "print(\"Best score is {}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute training metrics\n",
    "accuracy = optimized_clf.score(X_train, y_train)\n",
    "\n",
    "#  Predict labels of test set\n",
    "train_pred = optimized_clf.predict(X_train)\n",
    "\n",
    "# Compute MSE, confusion matrix, classification report\n",
    "mse = mean_squared_error(y_train, train_pred)\n",
    "conf_mat = confusion_matrix(y_train.round(), train_pred.round())\n",
    "clas_rep = classification_report(y_train.round(), train_pred.round())\n",
    "\n",
    "# Print reports\n",
    "print('{:=^80}'.format('Optimized LR training report'))\n",
    "print('Accuracy: %.4f' % accuracy)\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print(\"Confusion matrix:\\n{}\".format(conf_mat))\n",
    "print(\"Classification report:\\n{}\".format(clas_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute testing metrics\n",
    "accuracy = optimized_clf.score(X_test, y_test)\n",
    "\n",
    "# Predict labels of test set\n",
    "y_pred = optimized_clf.predict(X_test)\n",
    "\n",
    "# Compute MSE, confusion matrix, classification report\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test.round(), y_pred.round())\n",
    "clas_rep = classification_report(y_test.round(), y_pred.round())\n",
    "\n",
    "# Print reports\n",
    "print('{:=^80}'.format('Optimized LR testing report'))\n",
    "print('Accuracy: %.4f' % accuracy)\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print(\"Confusion matrix:\\n{}\".format(conf_mat))\n",
    "print(\"Classification report:\\n{}\".format(clas_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probabilities\n",
    "y_pred_prob = optimized_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculate receiver operating characteristics (ROC)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Compute AUC score\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k-')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Optimized LR Testing\\nROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute AUPRC score\n",
    "average_precision = average_precision_score(y_test, y_pred_prob)\n",
    "print(\"AUPRC: {}\".format(average_precision))\n",
    "\n",
    "# Plot PR curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Optimized LR Testing\\n2-class Precision-Recall curve: AP={0:0.4f}'.format(average_precision))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Logistic Regression final performance (above):* The model performance indicated by the AUPRC metric (area under Precision-Recall curve) did not improve substantially in comparison to the initial model, so it is time to try a different classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Support Vector Machine classification (SVC) <a id='three'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will fit a kernel-type support vector classifier (SVC) with Gaussian radial basis function (RBF). Where logistic regression uses the output of a linear model, the SVC will define a hyperplane within the N-dimensional parameter space to classify the data points into either of the two categories--1 for anomalous transactions and 0 for all other transactions. This parameter space consists of the set of N predictor or feature variables. We will follow the same step-wise approach as for LR, starting with an initial model followed by hyperparameter tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate classifier (turning off probability increases speed)\n",
    "clf = SVC(probability=True, gamma='scale', max_iter=-1, random_state=21)\n",
    "\n",
    "# Fit classifier to training set\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute training metrics\n",
    "accuracy = clf.score(X_train, y_train)\n",
    "\n",
    "#  Predict labels of test set\n",
    "train_pred = clf.predict(X_train)\n",
    "\n",
    "# Compute MSE, confusion matrix, classification report\n",
    "mse = mean_squared_error(y_train, train_pred)\n",
    "conf_mat = confusion_matrix(y_train.round(), train_pred.round())\n",
    "clas_rep = classification_report(y_train.round(), train_pred.round())\n",
    "\n",
    "# Print reports\n",
    "print('{:=^80}'.format('Initial SVC training report'))\n",
    "print('Accuracy: %.4f' % accuracy)\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print(\"Confusion matrix:\\n{}\".format(conf_mat))\n",
    "print(\"Classification report:\\n{}\".format(clas_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute testing metrics\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "# Predict labels of test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute MSE, confusion matrix, classification report\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test.round(), y_pred.round())\n",
    "clas_rep = classification_report(y_test.round(), y_pred.round())\n",
    "\n",
    "# Print reports\n",
    "print('{:=^80}'.format('Initial SVC testing report'))\n",
    "print('Accuracy: %.4f' % accuracy)\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print(\"Confusion matrix:\\n{}\".format(conf_mat))\n",
    "print(\"Classification report:\\n{}\".format(clas_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probabilities\n",
    "y_pred_prob = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculate receiver operating characteristics (ROC)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Compute AUC score\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k-')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Initial SVC Testing\\nROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute AUPRC score\n",
    "average_precision = average_precision_score(y_test, y_pred_prob)\n",
    "print(\"AUPRC: {}\".format(average_precision))\n",
    "\n",
    "# Plot PR curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Initial SVC Testing\\n2-class Precision-Recall curve: AP={0:0.4f}'.format(average_precision))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kernel SVC (above) performs better than LR because it predicts more true positives (anomalies recall=66%) and less false positives (anomalies precision=94%). At 81%, the SVC AUPRC, main metric of interest, is also greater than for LR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel SVC hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the Kernel SVC, we will tune parameters C and gamma. Gamma defines the nonlinear hyperplane of the SVC, and represents the inverse of the radius of influence of samples identified by the model as support vectors. Because gamma determines how closely the hyperplane fits the training set, it follows that high values of gamma can lead to overfitting. Therefore, we use a moderate range. Additionally, we limit the number of iterations in the event that the classifier does not converge toward a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "c_space = np.logspace(-3, 2, 51)\n",
    "gamma_space = np.logspace(-3, 2, 51)\n",
    "rand_grid = {'C': c_space,\n",
    "             'gamma': gamma_space}\n",
    "print(rand_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate RandomizedSearchCV object (use all cores but one)\n",
    "grid = RandomizedSearchCV(SVC(probability=True, random_state=21, max_iter = 1000), rand_grid,\n",
    "                          n_iter = 20, cv=2, random_state=21, n_jobs = -2, verbose = 2)\n",
    "\n",
    "# Fit object to data\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Extract best model\n",
    "optimized_clf = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the tuned parameters and score\n",
    "print('{:=^80}'.format('SVC parameters for best candidate'))\n",
    "print(\"Optimized Parameters: {}\".format(grid.best_params_)) \n",
    "print(\"All Parameters: {}\".format(optimized_clf.get_params())) \n",
    "print(\"Best score is {}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute training metrics\n",
    "accuracy = optimized_clf.score(X_train, y_train)\n",
    "\n",
    "#  Predict labels of test set\n",
    "train_pred = optimized_clf.predict(X_train)\n",
    "\n",
    "# Compute MSE, confusion matrix, classification report\n",
    "mse = mean_squared_error(y_train, train_pred)\n",
    "conf_mat = confusion_matrix(y_train.round(), train_pred.round())\n",
    "clas_rep = classification_report(y_train.round(), train_pred.round())\n",
    "\n",
    "# Print reports\n",
    "print('{:=^80}'.format('Optimized SVC training report'))\n",
    "print('Accuracy: %.4f' % accuracy)\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print(\"Confusion matrix:\\n{}\".format(conf_mat))\n",
    "print(\"Classification report:\\n{}\".format(clas_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute testing metrics\n",
    "accuracy = optimized_clf.score(X_test, y_test)\n",
    "\n",
    "# Predict labels of test set\n",
    "y_pred = optimized_clf.predict(X_test)\n",
    "\n",
    "# Compute MSE, confusion matrix, classification report\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test.round(), y_pred.round())\n",
    "clas_rep = classification_report(y_test.round(), y_pred.round())\n",
    "\n",
    "# Print reports\n",
    "print('{:=^80}'.format('Optimized SVC testing report'))\n",
    "print('Accuracy: %.4f' % accuracy)\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print(\"Confusion matrix:\\n{}\".format(conf_mat))\n",
    "print(\"Classification report:\\n{}\".format(clas_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probabilities\n",
    "y_pred_prob = optimized_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculate receiver operating characteristics (ROC)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Compute AUC score\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k-')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Optimized SVC Testing\\nROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute AUPRC score\n",
    "average_precision = average_precision_score(y_test, y_pred_prob)\n",
    "print(\"AUPRC: {}\".format(average_precision))\n",
    "\n",
    "# Plot PR curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Optimized SVC Testing\\n2-class Precision-Recall curve: AP={0:0.4f}'.format(average_precision))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Kernel Support Vector Classifier final performance (above):* \n",
    "\n",
    "* Hyperparameter tuning the Kernel SVC did not result in a better classifier than the Kernel SVC we started with. \n",
    "* Some Kernel SVC parameter combination failed to converge within the maximum number of iterations specified. \n",
    "* Regardless, Kernel SVC performed notably better than the LR in terms of precision (low number of false negatives and false positives), recall and area under Precision-Recall curve (AUPRC). \n",
    "\n",
    "At this point we could decide to try again and explore the parameter space in more detail, but let's try other classifiers instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Model (GBM) classification <a id='four'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting builds an additive model in a forward step-wise approach, by fitting a single regression tree (in binary classification) that optimizes the deviance loss function. As such, a GBM combines both parametric and non-parametric methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate classifier\n",
    "clf = GradientBoostingClassifier(random_state=21, verbose=1)\n",
    "\n",
    "# Fit classifier to the training set\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute training metrics\n",
    "accuracy = clf.score(X_train, y_train)\n",
    "\n",
    "#  Predict labels of test set\n",
    "train_pred = clf.predict(X_train)\n",
    "\n",
    "# Compute MSE, confusion matrix, classification report\n",
    "mse = mean_squared_error(y_train, train_pred)\n",
    "conf_mat = confusion_matrix(y_train.round(), train_pred.round())\n",
    "clas_rep = classification_report(y_train.round(), train_pred.round())\n",
    "\n",
    "# Print reports\n",
    "print('{:=^80}'.format('Initial SVC training report'))\n",
    "print('Accuracy: %.4f' % accuracy)\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print(\"Confusion matrix:\\n{}\".format(conf_mat))\n",
    "print(\"Classification report:\\n{}\".format(clas_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute testing metrics\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "# Predict labels of test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute MSE, confusion matrix, classification report\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test.round(), y_pred.round())\n",
    "clas_rep = classification_report(y_test.round(), y_pred.round())\n",
    "\n",
    "# Print reports\n",
    "print('{:=^80}'.format('Initial GBM testing report'))\n",
    "print('Accuracy: %.4f' % accuracy)\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print(\"Confusion matrix:\\n{}\".format(conf_mat))\n",
    "print(\"Classification report:\\n{}\".format(clas_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probabilities\n",
    "y_pred_prob = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculate receiver operating characteristics (ROC)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Compute AUC score\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k-')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Initial GBM Testing\\nROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute AUPRC score\n",
    "average_precision = average_precision_score(y_test, y_pred_prob)\n",
    "print(\"AUPRC: {}\".format(average_precision))\n",
    "\n",
    "# Plot PR curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Initial GBM Testing\\n2-class Precision-Recall curve: AP={0:0.4f}'.format(average_precision))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "learning_rate = [0.02, 0.1]\n",
    "n_estimators = [int(x) for x in [100, 200, 300, 400]]\n",
    "subsample = [0.5, 0.9]\n",
    "max_depth = [int(x) for x in [3, 4, 5, 10]]\n",
    "min_samples_split = [int(x) for x in [2, 3, 4, 5]]\n",
    "rand_grid = {'learning_rate': learning_rate,\n",
    "             'n_estimators': n_estimators,\n",
    "             'subsample': subsample,\n",
    "             'max_depth': max_depth,\n",
    "             'min_samples_split': min_samples_split}\n",
    "print(rand_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate RandomizedSearchCV object (use all cores but one)\n",
    "grid = RandomizedSearchCV(GradientBoostingClassifier(validation_fraction=0.3, n_iter_no_change=10, random_state=21), rand_grid, \n",
    "                          n_iter = 20, cv=2, random_state=21, n_jobs = -2, verbose = 2)\n",
    "\n",
    "# Fit object to data\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Extract best model\n",
    "optimized_clf = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the tuned parameters and score\n",
    "print('{:=^80}'.format('GBM parameters for best candidate'))\n",
    "print(\"Optimized Parameters: {}\".format(grid.best_params_)) \n",
    "print(\"All Parameters: {}\".format(optimized_clf.get_params())) \n",
    "print(\"Best score is {}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute training metrics\n",
    "accuracy = optimized_clf.score(X_train, y_train)\n",
    "\n",
    "#  Predict labels of test set\n",
    "train_pred = optimized_clf.predict(X_train)\n",
    "\n",
    "# Compute MSE, confusion matrix, classification report\n",
    "mse = mean_squared_error(y_train, train_pred)\n",
    "conf_mat = confusion_matrix(y_train.round(), train_pred.round())\n",
    "clas_rep = classification_report(y_train.round(), train_pred.round())\n",
    "\n",
    "# Print reports\n",
    "print('{:=^80}'.format('Optimized GBM training report'))\n",
    "print('Accuracy: %.4f' % accuracy)\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print(\"Confusion matrix:\\n{}\".format(conf_mat))\n",
    "print(\"Classification report:\\n{}\".format(clas_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute testing metrics\n",
    "accuracy = optimized_clf.score(X_test, y_test)\n",
    "\n",
    "# Predict labels of test set\n",
    "y_pred = optimized_clf.predict(X_test)\n",
    "\n",
    "# Compute MSE, confusion matrix, classification report\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test.round(), y_pred.round())\n",
    "clas_rep = classification_report(y_test.round(), y_pred.round())\n",
    "\n",
    "# Print reports\n",
    "print('{:=^80}'.format('Optimized GBM testing report'))\n",
    "print('Accuracy: %.4f' % accuracy)\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print(\"Confusion matrix:\\n{}\".format(conf_mat))\n",
    "print(\"Classification report:\\n{}\".format(clas_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probabilities\n",
    "y_pred_prob = optimized_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculate receiver operating characteristics (ROC)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Compute AUC score\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k-')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Optimized GBM Testing\\nROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute AUPRC score\n",
    "average_precision = average_precision_score(y_test, y_pred_prob)\n",
    "print(\"AUPRC: {}\".format(average_precision))\n",
    "\n",
    "# Plot PR curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Optimized GBM Testing\\n2-class Precision-Recall curve: AP={0:0.4f}'.format(average_precision))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an AUPRC of 0.7164 (above), the gradient boosting model performed worse than both the LR classifier and Kernel SVC. Lower performance is mostly explained by the lower recall for anomalies (value 1). While the GBM was on point for the anomalies it identified, it also missed a large percentage of anomalies--at least more than the LR classifier and Kernel SVC.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (RF) classification <a id='five'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now we used LR, Kernel SVC and GBM to make predictions, and all three have limitations when it comes to classification of unbalanced data. LR is a special case of generalized linear model (GLM) and makes assumptions about the underlying data distribution. This method works best with uncorrelated data and logarithmic error distributions, and therefore requires many data samples for fitting. Kernel SVC fitting involves the computation of polynomial surfaces, and the nonlinear nature of polynomial calculations makes that Kernel SVCs are not easily parallelized. GBM builds trees one at a time, each attempting to explain the residual error of the previous tree. While this can work for balanced datasets, the sequential nature of this process can be a disadvantage. \n",
    "\n",
    "Random Forest (RF) classification offers a non-parametric approach where a large number of uncorrelated models (decision trees) are fitted to the data, and vote independently as a joint committee on what the outcome of each prediction should be. One advantage of RF is that it makes no assumptions about the sample distribution or error distribution, meaning the classifier is robust and not biased by outliers. Furthermore, RF can be parallelized into as many processes as there are estimators (trees). Because a RF calculates fast, we will not hypertune the parameters but instead allow it to fit the data without constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit random forest for range of maximum depth of tree \n",
    "max_depths = [int(x) for x in [2,4,8,16,32,64]]\n",
    "\n",
    "train_results = []\n",
    "test_results = []\n",
    "for max_depth in max_depths:\n",
    "   clf = RandomForestClassifier(max_depth=max_depth, n_estimators=100, random_state=21, n_jobs = -2)\n",
    "   clf.fit(X_train, y_train)\n",
    "   train_pred_prob = clf.predict_proba(X_train)[:,1]\n",
    "   average_precision = average_precision_score(y_train, train_pred_prob)\n",
    "   train_results.append(average_precision)\n",
    "   y_pred_prob = clf.predict_proba(X_test)[:,1]\n",
    "   average_precision = average_precision_score(y_test, y_pred_prob)\n",
    "   test_results.append(average_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUPRC vs tree depth\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "line1, = plt.plot(max_depths, train_results, 'b', label=\"Train AUPRC\")\n",
    "line2, = plt.plot(max_depths, test_results, 'r', label=\"Test AUPRC\")\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('AUPRC score')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.title('Area under Precision-Recall curve vs Tree Depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum area under the Precision-Recall curve (AUPRC) is reached for a tree depth less than 20. It appears that an AUPRC of ~0.84 is the best possible testing performance we may expect given a training AUPRC of close to 1.00."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest without constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero ratios\n",
    "y_train_s = np.prod(y_train.shape)\n",
    "y_train_z = (y_train_s - np.sum(y_train)) / y_train_s\n",
    "y_test_s = np.prod(y_test.shape)\n",
    "y_test_z = (y_test_s - np.sum(y_test)) / y_test_s\n",
    "print(\"Zero ratio in training labels: {}\".format(y_train_z))\n",
    "print(\"Zero ratio in testing labels: {}\".format(y_test_z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sample weights for unbalanced classes as inverse of probability\n",
    "weight_0 = 1.0\n",
    "weight_1 = (1 - y_train_z)**-1\n",
    "sample_weight = np.array([weight_1 if i == 1 else weight_0 for i in enumerate(y_train)])\n",
    "print(\"Sample weight for logical(0): {}\".format(weight_0))\n",
    "print(\"Sample weight for logical(1): {}\".format(weight_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate classifier\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=21, n_jobs = -2, verbose = 2)\n",
    "\n",
    "# Fit classifier to training set\n",
    "clf = clf.fit(X_train, y_train, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute training metrics\n",
    "accuracy = clf.score(X_train, y_train)\n",
    "\n",
    "#  Predict labels of test set\n",
    "train_pred = clf.predict(X_train)\n",
    "\n",
    "# Compute MSE, confusion matrix, classification report\n",
    "mse = mean_squared_error(y_train, train_pred)\n",
    "conf_mat = confusion_matrix(y_train.round(), train_pred.round())\n",
    "clas_rep = classification_report(y_train.round(), train_pred.round())\n",
    "\n",
    "# Print reports\n",
    "print('{:=^80}'.format('RF training report'))\n",
    "print('Accuracy: %.4f' % accuracy)\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print(\"Confusion matrix:\\n{}\".format(conf_mat))\n",
    "print(\"Classification report:\\n{}\".format(clas_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute testing metrics\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "# Predict labels of test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute MSE, confusion matrix, classification report\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test.round(), y_pred.round())\n",
    "clas_rep = classification_report(y_test.round(), y_pred.round())\n",
    "\n",
    "# Print reports\n",
    "print('{:=^80}'.format('RF testing report'))\n",
    "print('Accuracy: %.4f' % accuracy)\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print(\"Confusion matrix:\\n{}\".format(conf_mat))\n",
    "print(\"Classification report:\\n{}\".format(clas_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probabilities\n",
    "y_pred_prob = clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate receiver operating characteristics (ROC)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Compute AUC score\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k-')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('RF Testing\\nROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute AUPRC score\n",
    "average_precision = average_precision_score(y_test, y_pred_prob)\n",
    "print(\"AUPRC: {}\".format(average_precision))\n",
    "\n",
    "# Plot PR curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('RF Testing\\n2-class Precision-Recall curve: AP={0:0.4f}'.format(average_precision))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection and cost-effective optimization <a id='six'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Selecting the best classifier:* We have tested four classifiers: Logistic Regression, Kernel Support Vector Classifier, Stochastic Gradient Boosting and Random Forest. Which one is the best choice? A practical way to evaluate the suitability of a classifier for daily use is in terms of cost-effectiveness: we want the classifier to be able to make predictions on new data (high testing precision and recall), and at a low cost (fast computation and modest data requirements). \n",
    "\n",
    "The Random Forest (AUPRC=0.8456) performed best in terms of precision and recall, followed by Kernel Support Vector Classifier (AUPRC=0.8081), Logistic Regression (AUPRC=0.7822) and Stochastic Gradient Boosting Machine (AUPRC=0.7164). RF and LR computed fastest. Conversely, Kernel SVC used the most computer time. While in a real-world scenario we might be able to obtain a better model (probably SGB or RF) given enough training data and more iterations, we assume that RF provides the most cost-effective prediction of transaction anomalies for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have identified the RF as the most suitable model to predict transaction anomalies, the next step is to see if can improve its cost-effectiveness by reducing data requirements. We trained the model on 29 features, so let's find out which of those features contribute the most information to the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_list = list(df.columns[:-1])\n",
    "importances = list(clf.feature_importances_)\n",
    "feature_importances = [(feature, round(importance, 4)) for feature, importance in zip(feature_list, importances)]\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "_ = [print('Variable: {:3} Importance: {}'.format(*pair)) for pair in feature_importances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature ranking in bar chart\n",
    "X_values = list(range(len(importances)))\n",
    "plt.bar(X_values, importances, orientation = 'vertical', color = 'r', edgecolor = 'k', linewidth = 1.2)\n",
    "plt.xticks(X_values, feature_list, rotation = 'vertical')\n",
    "plt.ylabel('Importance')\n",
    "plt.xlabel('Variable')\n",
    "plt.title('Feature Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features sorted by decreasing importance\n",
    "sorted_importances = [importance[1] for importance in feature_importances]\n",
    "sorted_features = [importance[0] for importance in feature_importances]\n",
    "\n",
    "# Cumulative importance\n",
    "cumulative_importances = np.cumsum(sorted_importances)\n",
    "\n",
    "# Create line plot\n",
    "plt.plot(X_values, cumulative_importances, 'b-')\n",
    "plt.hlines(y = 0.95, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n",
    "plt.xticks(X_values, sorted_features, rotation = 'vertical')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Cumulative Importance')\n",
    "plt.title('Feature Cumulative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features explaining 95% cum. importance\n",
    "n_import = np.where(cumulative_importances > 0.95)[0][0] + 1\n",
    "print('Number of features required (95% importance):', n_import)\n",
    "\n",
    "# Least important features\n",
    "limp_feature_names = sorted_features[-(len(importances)-n_import):]\n",
    "print('Least important features (5% importance):', limp_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance analysis (above) shows that 24 features out of the available set of 29 features account for 95% of the Gini Importance. There are three things we learn from this: \n",
    "\n",
    "1. There is a relative lack of correlation between features, confirming that these are indeed orthogonally transformed data (in this case the outcome of principal component analysis); \n",
    "2. The RF classifier makes optimum use of the training data; \n",
    "3. We can drop 5 of the features without incurring a high cost to model performance, namely the PCAs labeled as 28, 13, 24, 25 and 23. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain classifier on the most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the names of most important features\n",
    "important_feature_names = [feature[0] for feature in feature_importances[0:(n_import - 1)]]\n",
    "\n",
    "# Find the columns of the most important features\n",
    "important_indices = [feature_list.index(feature) for feature in important_feature_names]\n",
    "\n",
    "# Create training and testing sets with only important features\n",
    "X_train_imp = X_train[:,important_indices]\n",
    "X_test_imp = X_test[:,important_indices]\n",
    "\n",
    "# Print dimensions\n",
    "print(\"Dimensions of X_train_imp: {}\".format(X_train_imp.shape))\n",
    "print(\"Dimensions of y_train_imp: {}\".format(y_train.shape))\n",
    "print(\"Dimensions of X_test_imp: {}\".format(X_test_imp.shape))\n",
    "print(\"Dimensions of y_test_imp: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit classifier to training set\n",
    "clf = clf.fit(X_train_imp, y_train, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute training metrics\n",
    "accuracy = clf.score(X_train_imp, y_train)\n",
    "\n",
    "#  Predict labels of test set\n",
    "train_pred = clf.predict(X_train_imp)\n",
    "\n",
    "# Compute MSE, confusion matrix, classification report\n",
    "mse = mean_squared_error(y_train, train_pred)\n",
    "conf_mat = confusion_matrix(y_train.round(), train_pred.round())\n",
    "clas_rep = classification_report(y_train.round(), train_pred.round())\n",
    "\n",
    "# Print reports\n",
    "print('{:=^80}'.format('New RF training report'))\n",
    "print('Accuracy: %.4f' % accuracy)\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print(\"Confusion matrix:\\n{}\".format(conf_mat))\n",
    "print(\"Classification report:\\n{}\".format(clas_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute testing metrics\n",
    "accuracy = clf.score(X_test_imp, y_test)\n",
    "\n",
    "# Predict labels of test set\n",
    "y_pred = clf.predict(X_test_imp)\n",
    "\n",
    "# Compute MSE, confusion matrix, classification report\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test.round(), y_pred.round())\n",
    "clas_rep = classification_report(y_test.round(), y_pred.round())\n",
    "\n",
    "# Print reports\n",
    "print('{:=^80}'.format('New RF testing report'))\n",
    "print('Accuracy: %.4f' % accuracy)\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print(\"Confusion matrix:\\n{}\".format(conf_mat))\n",
    "print(\"Classification report:\\n{}\".format(clas_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probabilities\n",
    "y_pred_prob = clf.predict_proba(X_test_imp)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute AUPRC score\n",
    "average_precision = average_precision_score(y_test, y_pred_prob)\n",
    "print(\"AUPRC: {}\".format(average_precision))\n",
    "\n",
    "# Plot PR curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('New RF Testing\\n2-class Precision-Recall curve: AP={0:0.4f}'.format(average_precision))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing AUPRC has not decreased after retraining the RF with the 24 most important of 29 features. We now have a more cost-effective RF classifier that has the same predictive power, but requires less data to train. These are the final scores:  \n",
    " \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Classifier</th>\n",
    "    <th>AUPRC</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Random Forest (24 most important features)</td>\n",
    "    <td>0.8487</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Random Forest (all 29 features)</td>\n",
    "    <td>0.8456</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Kernel Support Vector Classifier</td>\n",
    "    <td>0.8081</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Logistic Regression</td>\n",
    "    <td>0.7822</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Stochastic Boosted Regression</td>\n",
    "    <td>0.7164</td>\n",
    "  </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion <a id='seven'></a> \n",
    "\n",
    "* Conventional metrics for classification models like accuracy and mean squared error give a too optimistic impression of model performance. Because the transaction dataset is a highly unbalanced dataset (anomalous transactions accounted for 0.1727% of all transactions), the area under Precision-Recall curve (AUPRC) is a more useful metric of model performance. \n",
    "\n",
    "* The Random Forest provided the most cost-effective anomaly detection in terms of precision and recall (AUPRC=0.8487) and computation time, followed by the fast Logistic Regression (AUPRC=0.7822) and slower Kernel Support Vector Classifier (AUPRC=0.8081) and Stochastic Gradient Boosting Machine (AUPRC=0.7164). \n",
    "\n",
    "* 5 of the original 29 features can be dropped without loss of model performance.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
